{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning Project 1 : Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is a description of our environment and algorithms we use to train an agent that learns by himself (trials and errors) to navigate and collect a type of bananas in a large, square world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. The task is episodic, and in order to solve the environment, our agent must get an average score of +13 over 100 consecutive episodes. Our problem is well framed to be categorized as a Reinforcement Learning problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we have used [Unity ML Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#banana-collector)'s rich environments to design, train, and evaluate our Deep Reinforcement learning algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Unity Academy name: Academy\n",
    "        - Number of Brains: 1\n",
    "\n",
    "+ Unity brain name: BananaBrain\n",
    "        - Number of Visual Observations (per agent): 0\n",
    "        - Vector Observation space type: continuous\n",
    "        - Vector Observation space size (per agent): 37\n",
    "        - Number of stacked Vector Observation: 1\n",
    "        - Vector Action space type: discrete\n",
    "        - Vector Action space size (per agent): 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem we use a Deep Q-Networks (DQN). we are given 7 continuous states, 4 discrete actions so we used three fully-connected layer and two Rectified Linear Units. Below is the model implementation in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stabilizing Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is notoriously unstable when neural networks are used to represent the action values. Deep Q Learning algorithms adressed these instabilities by using two key features.\n",
    "    \n",
    "Training Techniques:\n",
    "    - Experience Replay\n",
    "    - Fixed Q-Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay enables RL agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. When the agent interacts with the environment, the sequence of experience tuples can be highly correlated. The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation. By instead keeping track of a **replay buffer** and using **experience replay** to sample from the buffer at random, we can prevent action values from oscillating or diverging catastrophically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **replay buffer** contains a collection of experience tuples *(S, A, R, Sâ€²)*. The tuples are gradually added to the buffer as we are interacting with the environment. We use a buffer size of 1e5 . The tuples are gradually added to the buffer as we are interacting with the environment.\n",
    "\n",
    "The act of sampling a small batch of tuples from the replay buffer in order to learn is known as **experience replay**. In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.\n",
    "\n",
    "We used a buffer size of **1e5**, meaning that we store up to 1e5 experience tuples at a time from which we sample a mini-batch of **64** experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Fixed Q-Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without fixed Q-Targets, we would encounter a harmful form of correlation, whereby we shift the parameters of the network based on a constantly moving targer. As for the other parameters involved in the update, the **learning rate** is initialized at\n",
    "**5e-4** and we used **Adam optimizer**. The discount rate is set to 0.99 with is closer to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plotted Rewards\n",
    "\n",
    "The problem is solved in 487 episodes. The plot below shows the reward per epidode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Episode 100\tAverage Score: 1.12\n",
    "Episode 200\tAverage Score: 4.88\n",
    "Episode 300\tAverage Score: 8.34\n",
    "Episode 400\tAverage Score: 10.43\n",
    "Episode 487\tAverage Score: 13.03\n",
    "Environment solved in 487 episodes!\t Average score: 13.03\n",
    "Training Time is 620.8464601039886\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"> \n",
    "    <img src=\"Assets/plot.png\" align=\"left\" alt=\"drawing\" width=\"700px\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Future works\n",
    "\n",
    ">[Implement a Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    ">[Implement a Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1509.06461)\n",
    "\n",
    ">[Implement Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
